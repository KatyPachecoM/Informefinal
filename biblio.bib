
@article{de_la_fuente-mella_econometric_2020,
	title = {Econometric modeling of productivity and technical efficiency in the {Chilean} manufacturing industry},
	volume = {139},
	issn = {0360-8352},
	url = {http://www.sciencedirect.com/science/article/pii/S0360835219302098},
	doi = {10.1016/j.cie.2019.04.006},
	abstract = {This research presents models for assessing productivity and technical efficiency of the Chilean manufacturing sector by using a factor analysis as a methodological alternative to the standard model of productive factors, generating latent factors or input variables such as those proposed in the literature on the topic. The factor analysis allows us to synthesize the number of inputs available as latent factors, showing satisfactory quite results. Econometric and stochastic frontier analyses are performed (i) to determine the influential factors of the Chilean manufacturing output and (ii) to evaluate the levels of technical efficiency for each Chilean industrial sector. The results obtained show that, for both the standard model and the proposed factorial model, the economic theory is validated in terms of the importance of the inputs that form the manufacturing outputs. Moreover, a functional structure is defined as an adequate representation of the manufacturing outputs in Chile. The manufacturing sector of the country presents a downward trend in technical efficiency during the study period, with higher decline rates in the lowest efficient ratings. The results show a period of productivity growth from 1986 to 1996, a slowdown from 1998, and only a slight increase in 2004. Similarly, important intra-group variations in the manufacturing sector are observed, especially in the of food group, in which average efficiencies from 19\% to 91\% are reported. Innovative discussion, implications and recommendations concerning our investigation, as well as issues associated with industrial policies, are provided and suited within the “Industry 4.0” context.},
	language = {en},
	urldate = {2020-07-19},
	journal = {Computers \& Industrial Engineering},
	author = {de la Fuente-Mella, Hanns and Rojas Fuentes, José Luis and Leiva, Víctor},
	month = jan,
	year = {2020},
	keywords = {Cobb-Douglas and trans-logarithmic models, Cronbach coefficient, Data-driven decision making, Industry 4.0, Stochastic frontier analysis, Supply chain},
	pages = {105793},
	file = {ScienceDirect Snapshot:C\:\\Users\\USUARIO\\Zotero\\storage\\KLGIMCGK\\S0360835219302098.html:text/html;ScienceDirect Full Text PDF:C\:\\Users\\USUARIO\\Zotero\\storage\\TSN93IS5\\de la Fuente-Mella et al. - 2020 - Econometric modeling of productivity and technical.pdf:application/pdf}
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Forecasting} performance measures for traffic safety using deterministic and stochastic models},
	url = {https://www.researchgate.net/publication/308864167_Forecasting_performance_measures_for_traffic_safety_using_deterministic_and_stochastic_models},
	urldate = {2020-07-19},
	file = {(PDF) Forecasting performance measures for traffic safety using deterministic and stochastic models:C\:\\Users\\USUARIO\\Zotero\\storage\\CP7E2IQH\\308864167_Forecasting_performance_measures_for_traffic_safety_using_deterministic_and_stochasti.html:text/html}
}

@article{fuente-mella_econometric_2013,
	title = {An {Econometric} {Analysis} for the {Behavior} of the {Bid}-{Ask} {Spread}},
	volume = {1},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	url = {http://www.scirp.org/Journal/Paperabs.aspx?paperid=40702},
	doi = {10.4236/jss.2013.17001},
	abstract = {Information asymmetries are an important element in the functioning of capital markets. An indirect means of measuring information asymmetry is through the spread of stock prices. The purpose of this paper is to identify the explanatory variables and the determinants of the bid-ask spread and to quantify the influence that the actors involved in the brokering of publically offered securities may have over the spread. The methodology used to model the time series for each of the analyzed companies is based on a time series from each of the observed econometric multivariate processes. The analysis shows a significantly negative relationship between the spread and the market-maker size, calculated in terms of both the equity and the stock portfolio; likewise, activity is measured by observing the amount offered for purchase and/or sale.},
	language = {en},
	number = {7},
	urldate = {2020-07-19},
	journal = {Open Journal of Social Sciences},
	author = {Fuente-Mella, H. de la and Campos-Espinoza, R. and Silva-Palavecinos, B. and Cademartori-Rosso, D.},
	month = dec,
	year = {2013},
	note = {Number: 7
Publisher: Scientific Research Publishing},
	pages = {1--5},
	file = {Full Text PDF:C\:\\Users\\USUARIO\\Zotero\\storage\\8LE9G2E6\\Fuente-Mella et al. - 2013 - An Econometric Analysis for the Behavior of the Bi.pdf:application/pdf;Snapshot:C\:\\Users\\USUARIO\\Zotero\\storage\\S74FJ6J7\\paperinformation.html:text/html}
}

@incollection{heckman_chapter_2007,
	title = {Chapter 70 {Econometric} {Evaluation} of {Social} {Programs}, {Part} {I}: {Causal} {Models}, {Structural} {Models} and {Econometric} {Policy} {Evaluation}},
	volume = {6},
	shorttitle = {Chapter 70 {Econometric} {Evaluation} of {Social} {Programs}, {Part} {I}},
	url = {http://www.sciencedirect.com/science/article/pii/S1573441207060709},
	abstract = {This chapter relates the literature on the econometric evaluation of social programs to the literature in statistics on “causal inference”. In it, we develop a general evaluation framework that addresses well-posed economic questions and analyzes agent choice rules and subjective evaluations of outcomes as well as the standard objective evaluations of outcomes. The framework recognizes uncertainty faced by agents and ex ante and ex post evaluations of programs. It also considers distributions of treatment effects. These features are absent from the statistical literature on causal inference. A prototypical model of agent choice and outcomes is used to illustrate the main ideas. We formally develop models for counterfactuals and causality that build on Cowles Commission econometrics. These models anticipate and extend the literature on causal inference in statistics. The distinction between fixing and conditioning that has recently entered the statistical literature was first developed by Cowles economists. Models of simultaneous causality were also developed by the Cowles group, as were notions of invariance to policy interventions. These basic notions are updated to nonlinear and nonparametric frameworks for policy evaluation more general than anything in the current statistical literature on “causal inference”. A formal discussion of identification is presented and applied to clearly formulated choice models used to evaluate social programs.},
	language = {en},
	urldate = {2020-07-19},
	booktitle = {Handbook of {Econometrics}},
	publisher = {Elsevier},
	author = {Heckman, James J. and Vytlacil, Edward J.},
	editor = {Heckman, James J. and Leamer, Edward E.},
	month = jan,
	year = {2007},
	doi = {10.1016/S1573-4412(07)06070-9},
	keywords = {causal models, counterfactuals, identification, policy evaluation, policy invariance, structural models},
	pages = {4779--4874},
	file = {ScienceDirect Snapshot:C\:\\Users\\USUARIO\\Zotero\\storage\\86XNKYB3\\S1573441207060709.html:text/html;Texto completo:C\:\\Users\\USUARIO\\Zotero\\storage\\4NJXSSYC\\Heckman y Vytlacil - 2007 - Chapter 70 Econometric Evaluation of Social Progra.pdf:application/pdf}
}

@misc{noauthor_insertar_2018,
	title = {Insertar referencias bibliográficas en {R} {Markdown} desde {Zotero} con citr},
	url = {https://vivaelsoftwarelibre.com/insertar-referencias-bibliograficas-en-r-markdown/},
	abstract = {El paquete de R llamado citr nos ayuda a insertar referencias bibliográficas en R Markdown a partir de Zotero. En esta entrada os explicamos cómo.},
	language = {es},
	urldate = {2020-07-21},
	journal = {Viva el Software Libre},
	month = oct,
	year = {2018},
	note = {Library Catalog: vivaelsoftwarelibre.com}
}

@incollection{heckman_chapter_2007,
	title = {Chapter 70 {Econometric} {Evaluation} of {Social} {Programs}, {Part} {I}: {Causal} {Models}, {Structural} {Models} and {Econometric} {Policy} {Evaluation}},
	volume = {6},
	shorttitle = {Chapter 70 {Econometric} {Evaluation} of {Social} {Programs}, {Part} {I}},
	url = {http://www.sciencedirect.com/science/article/pii/S1573441207060709},
	abstract = {This chapter relates the literature on the econometric evaluation of social programs to the literature in statistics on “causal inference”. In it, we develop a general evaluation framework that addresses well-posed economic questions and analyzes agent choice rules and subjective evaluations of outcomes as well as the standard objective evaluations of outcomes. The framework recognizes uncertainty faced by agents and ex ante and ex post evaluations of programs. It also considers distributions of treatment effects. These features are absent from the statistical literature on causal inference. A prototypical model of agent choice and outcomes is used to illustrate the main ideas. We formally develop models for counterfactuals and causality that build on Cowles Commission econometrics. These models anticipate and extend the literature on causal inference in statistics. The distinction between fixing and conditioning that has recently entered the statistical literature was first developed by Cowles economists. Models of simultaneous causality were also developed by the Cowles group, as were notions of invariance to policy interventions. These basic notions are updated to nonlinear and nonparametric frameworks for policy evaluation more general than anything in the current statistical literature on “causal inference”. A formal discussion of identification is presented and applied to clearly formulated choice models used to evaluate social programs.},
	language = {en},
	urldate = {2020-07-19},
	booktitle = {Handbook of {Econometrics}},
	publisher = {Elsevier},
	author = {Heckman, James J. and Vytlacil, Edward J.},
	editor = {Heckman, James J. and Leamer, Edward E.},
	month = jan,
	year = {2007},
	doi = {10.1016/S1573-4412(07)06070-9},
	keywords = {causal models, counterfactuals, identification, policy evaluation, policy invariance, structural models},
	pages = {4779--4874},
	file = {ScienceDirect Snapshot:C\:\\Users\\USUARIO\\Zotero\\storage\\86XNKYB3\\S1573441207060709.html:text/html;Texto completo:C\:\\Users\\USUARIO\\Zotero\\storage\\4NJXSSYC\\Heckman y Vytlacil - 2007 - Chapter 70 Econometric Evaluation of Social Progra.pdf:application/pdf}
}

@InProceedings{paz_forecasting_2015,
  title = {Forecasting Performance Measures for Traffic Safety Using Deterministic and Stochastic Models},
  booktitle = {2015 {{IEEE}} 18th {{International Conference}} on {{Intelligent Transportation Systems}}},
  author = {Alexander Paz and Naveen Veeramisti and Hanns {de la Fuente-Mella}},
  year = {2015},
  month = {sep},
  pages = {2965--2970},
  issn = {2153-0017},
  doi = {10.1109/ITSC.2015.475},
  abstract = {Traffic-safety performance measures required by the Moving Ahead Progress in 21st Century (MAP-21) act were forecasted in this study to facilitate the reduction of fatalities and serious injuries. Given the lack of exposure data (e.g., traffic counts), time series were used to conduct the forecast. Deterministic and stochastic models were applied using four independent and univariate time series from the crash data collected by the Nevada Department of Transportation. The best model specification was obtained using root mean square error and mean absolute percent prediction error as goodness of fit. Among several deterministic models evaluated in this study, the Winter-additive model for seasonal data and the Damped-trend model for non-seasonal data provided adequate forecasts. In the case of stochastic models, for non-seasonal data, an Autoregressive Integrated Moving Average (ARIMA) model provided acceptable results. However, the absence of adequate data likely precludes an appropriate estimation using the ARIMA model. For seasonal data, a Seasonal Autoregressive Integrated Moving Average (SARIMA) model provided the best forecast measures. The stochastic SARIMA(0,0,5)(0,1,1)12 model, an improved model, had a preferred fit for predicting the number of fatalities and serious injuries in Nevada over a five-year horizon. The SARIMA model could be an appropriate statistical model to predict fatalities and serious injuries as required by MAP-21.},
  file = {/Users/manuelayala/Zotero/storage/BPBYV6VB/Paz et al. - 2015 - Forecasting performance measures for traffic safet.pdf;/Users/manuelayala/Zotero/storage/4N2QFXH8/7313568.html},
  keywords = {ARIMA,autoregressive moving average processes,crash data,crash forecast,crash prediction,Data models,deterministic models,fatalities,Forecasting,forecasting theory,Injuries,MAP-21,Market research,moving ahead progress,Nevada Department of Transportation,performance measure forecasting,Predictive models,road safety,road traffic,Safety,SARIMA,SARIMA model,seasonal autoregressive integrated moving average,serious injuries,stochastic models,time series,traffic safety,traffic-safety performance measures,transportation,univariate time series},
}
